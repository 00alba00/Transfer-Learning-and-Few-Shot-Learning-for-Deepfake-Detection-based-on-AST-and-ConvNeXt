{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bb0cbb-a507-4642-8aac-2897d903d947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from transformers import ASTFeatureExtractor, ASTForAudioClassification\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2debeb74-e51e-4324-8dfd-a526a4717251",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {\"bonafide\": 1, \"spoof\": 0}\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(\n",
    "    \"...\"\n",
    ")\n",
    "sampling_rate = 16000\n",
    "\n",
    "\n",
    "class ASTDataset(Dataset):\n",
    "    def __init__(self, csv_path, audio_dir, nrows=None):\n",
    "        self.df = pd.read_csv(csv_path, sep=\" \", header=None, nrows=nrows)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.labels = [LABEL_MAP[label] for label in self.df[4]]  # precompute labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        audio_filename = row[1] + \".flac\"\n",
    "        label_str = row[4]\n",
    "        label = LABEL_MAP[label_str]\n",
    "\n",
    "        flac_path = os.path.join(self.audio_dir, audio_filename)\n",
    "        waveform, sr = torchaudio.load(flac_path)\n",
    "\n",
    "        # Convert to mono\n",
    "        waveform = waveform.mean(dim=0)\n",
    "\n",
    "        # Resample to 16kHz if needed\n",
    "        if sr != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        return {\n",
    "            \"waveform\": waveform.numpy(),  # to use in collate_fn\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def ast_collate_fn(batch):\n",
    "    waveforms = [item[\"waveform\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    inputs = feature_extractor(\n",
    "        waveforms,\n",
    "        sampling_rate=16000,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_values\": inputs[\"input_values\"],\n",
    "        \"labels\": torch.stack(labels)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a068edbd-3bf7-4394-bef3-1a150696a393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_dataset = ASTDataset(\n",
    "    csv_path=\"/root/autodl-fs/ASVspoof2019.LA.cm.train.trn.txt\",\n",
    "    audio_dir=\"/root/autodl-fs/ASVspoof2019_LA_train/flac\",\n",
    ")\n",
    "\n",
    "count_0 = 22800\n",
    "count_1 = 2580\n",
    "total = count_0 + count_1\n",
    "\n",
    "# Inverse frequency\n",
    "class_weights = {\n",
    "    0: total / count_0, \n",
    "    1: total / count_1   \n",
    "}\n",
    "\n",
    "# Get all labels from the dataset\n",
    "labels = train_dataset.get_labels()\n",
    "\n",
    "# Assign sample weights\n",
    "sample_weights = [class_weights[label] for label in labels]\n",
    "\n",
    "\n",
    "# Create the sampler\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=45600,\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=ast_collate_fn)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    sampler=sampler,\n",
    "    collate_fn=ast_collate_fn\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch[\"input_values\"].shape)  \n",
    "    print(batch[\"labels\"])\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa518f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = ASTDataset(\n",
    "    csv_path=\"/root/autodl-fs/ASVspoof2019.LA.cm.dev.trl.txt\",\n",
    "    audio_dir=\"/root/autodl-fs/ASVspoof2019_LA_dev/flac\",\n",
    ")\n",
    "\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=8, shuffle=True, collate_fn=ast_collate_fn)\n",
    "\n",
    "for batch in dev_dataloader:\n",
    "    print(batch[\"input_values\"].shape) \n",
    "    print(batch[\"labels\"])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = ASTDataset(\n",
    "    csv_path=\"/root/autodl-fs/ASVspoof2019.LA.cm.eval.trl.txt\",\n",
    "    audio_dir=\"/root/autodl-tmp/ASVspoof2019_LA_eval/flac\",\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8, shuffle=True, collate_fn=ast_collate_fn)\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    print(batch[\"input_values\"].shape)  \n",
    "    print(batch[\"labels\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8d7b5-e20e-4fd7-9de0-e082fab24028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at /root/autodl-fs/ast-audioset and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"...\"\n",
    "model = ASTForAudioClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,                      # adjust for your task\n",
    "    ignore_mismatched_sizes=True      # useful when num_labels differs\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dadbcb8-215d-4fbe-886c-1f1e37df7681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_spectrogram_transformer.embeddings.cls_token False\n",
      "audio_spectrogram_transformer.embeddings.distillation_token False\n",
      "audio_spectrogram_transformer.embeddings.position_embeddings False\n",
      "audio_spectrogram_transformer.embeddings.patch_embeddings.projection.weight False\n",
      "audio_spectrogram_transformer.embeddings.patch_embeddings.projection.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.0.attention.attention.query.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.0.attention.attention.query.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.0.attention.attention.key.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.0.attention.attention.key.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.0.attention.attention.value.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.0.attention.attention.value.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.0.attention.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.0.attention.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.0.intermediate.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.0.intermediate.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.0.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.0.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.0.layernorm_before.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.0.layernorm_before.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.0.layernorm_after.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.0.layernorm_after.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.1.attention.attention.query.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.1.attention.attention.query.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.1.attention.attention.key.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.1.attention.attention.key.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.1.attention.attention.value.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.1.attention.attention.value.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.1.attention.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.1.attention.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.1.intermediate.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.1.intermediate.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.1.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.1.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.1.layernorm_before.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.1.layernorm_before.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.1.layernorm_after.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.1.layernorm_after.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.2.attention.attention.query.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.2.attention.attention.query.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.2.attention.attention.key.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.2.attention.attention.key.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.2.attention.attention.value.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.2.attention.attention.value.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.2.attention.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.2.attention.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.2.intermediate.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.2.intermediate.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.2.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.2.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.2.layernorm_before.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.2.layernorm_before.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.2.layernorm_after.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.2.layernorm_after.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.3.attention.attention.query.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.3.attention.attention.query.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.3.attention.attention.key.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.3.attention.attention.key.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.3.attention.attention.value.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.3.attention.attention.value.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.3.attention.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.3.attention.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.3.intermediate.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.3.intermediate.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.3.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.3.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.3.layernorm_before.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.3.layernorm_before.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.3.layernorm_after.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.3.layernorm_after.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.4.attention.attention.query.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.4.attention.attention.query.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.4.attention.attention.key.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.4.attention.attention.key.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.4.attention.attention.value.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.4.attention.attention.value.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.4.attention.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.4.attention.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.4.intermediate.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.4.intermediate.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.4.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.4.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.4.layernorm_before.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.4.layernorm_before.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.4.layernorm_after.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.4.layernorm_after.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.5.attention.attention.query.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.5.attention.attention.query.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.5.attention.attention.key.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.5.attention.attention.key.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.5.attention.attention.value.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.5.attention.attention.value.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.5.attention.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.5.attention.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.5.intermediate.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.5.intermediate.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.5.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.5.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.5.layernorm_before.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.5.layernorm_before.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.5.layernorm_after.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.5.layernorm_after.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.6.attention.attention.query.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.6.attention.attention.query.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.6.attention.attention.key.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.6.attention.attention.key.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.6.attention.attention.value.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.6.attention.attention.value.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.6.attention.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.6.attention.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.6.intermediate.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.6.intermediate.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.6.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.6.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.6.layernorm_before.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.6.layernorm_before.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.6.layernorm_after.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.6.layernorm_after.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.7.attention.attention.query.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.7.attention.attention.query.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.7.attention.attention.key.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.7.attention.attention.key.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.7.attention.attention.value.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.7.attention.attention.value.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.7.attention.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.7.attention.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.7.intermediate.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.7.intermediate.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.7.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.7.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.7.layernorm_before.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.7.layernorm_before.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.7.layernorm_after.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.7.layernorm_after.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.8.attention.attention.query.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.8.attention.attention.query.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.8.attention.attention.key.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.8.attention.attention.key.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.8.attention.attention.value.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.8.attention.attention.value.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.8.attention.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.8.attention.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.8.intermediate.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.8.intermediate.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.8.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.8.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.8.layernorm_before.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.8.layernorm_before.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.8.layernorm_after.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.8.layernorm_after.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.9.attention.attention.query.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.9.attention.attention.query.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.9.attention.attention.key.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.9.attention.attention.key.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.9.attention.attention.value.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.9.attention.attention.value.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.9.attention.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.9.attention.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.9.intermediate.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.9.intermediate.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.9.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.9.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.9.layernorm_before.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.9.layernorm_before.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.9.layernorm_after.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.9.layernorm_after.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.10.attention.attention.query.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.10.attention.attention.query.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.10.attention.attention.key.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.10.attention.attention.key.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.10.attention.attention.value.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.10.attention.attention.value.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.10.attention.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.10.attention.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.10.intermediate.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.10.intermediate.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.10.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.10.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.10.layernorm_before.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.10.layernorm_before.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.10.layernorm_after.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.10.layernorm_after.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.11.attention.attention.query.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.11.attention.attention.query.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.11.attention.attention.key.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.11.attention.attention.key.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.11.attention.attention.value.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.11.attention.attention.value.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.11.attention.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.11.attention.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.11.intermediate.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.11.intermediate.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.11.output.dense.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.11.output.dense.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.11.layernorm_before.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.11.layernorm_before.bias False\n",
      "audio_spectrogram_transformer.encoder.layer.11.layernorm_after.weight False\n",
      "audio_spectrogram_transformer.encoder.layer.11.layernorm_after.bias False\n",
      "audio_spectrogram_transformer.layernorm.weight False\n",
      "audio_spectrogram_transformer.layernorm.bias False\n",
      "classifier.layernorm.weight True\n",
      "classifier.layernorm.bias True\n",
      "classifier.dense.weight True\n",
      "classifier.dense.bias True\n"
     ]
    }
   ],
   "source": [
    "# Freeze layers (Linear Probing)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' in name:\n",
    "        param.requires_grad = True\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "    \n",
    "# Partial tuning\n",
    "# Freeze first 6 transformer encoder layers\n",
    "for name, param in model.named_parameters():\n",
    "    if any(f\"encoder.layer.{i}.\" in name for i in range(6)):\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True  # Unfreeze the rest\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "\n",
    "# Full tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abbadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/root/autodl-fs/ast-audioset\"\n",
    "model = ASTForAudioClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,                      # adjust for your task\n",
    "    ignore_mismatched_sizes=True      # useful when num_labels differs\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "\n",
    "# weighted loss\n",
    "# comput weight\n",
    "\n",
    "\n",
    "count_0 = 22800  # spoof\n",
    "count_1 = 2580   # bonafide\n",
    "total = count_0 + count_1\n",
    "\n",
    "# # Inverse frequency weights\n",
    "# weight_0 = total / count_0  # ≈ 1.1877\n",
    "# weight_1 = total / count_1  # ≈ 6.3096\n",
    "\n",
    "import math\n",
    "weight_0 = 1.0 / math.sqrt(count_0)\n",
    "weight_1 = 1.0 / math.sqrt(count_1)\n",
    "\n",
    "# Convert to tensor\n",
    "class_weights = torch.tensor([weight_0, weight_1], dtype=torch.float).to(device)\n",
    "\n",
    "# Define weighted loss\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385561d-06d7-411a-bb0b-c59c15ebb9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eer(y_true, y_score):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    eer_idx = np.nanargmin(np.abs(fnr - fpr))\n",
    "    eer = (fpr[eer_idx] + fnr[eer_idx]) / 2\n",
    "    return eer\n",
    "\n",
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    y_true, y_score, y_pred = [], [], []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        x = batch[\"input_values\"].to(device)\n",
    "        y = batch[\"labels\"].to(device)\n",
    "\n",
    "        out = model(input_values=x)  # ✅ AST input\n",
    "        logits = out.logits\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1]  # class 1 (bonafide)\n",
    "        y_true.extend(y.cpu().numpy())\n",
    "        y_score.extend(probs.detach().cpu().numpy())\n",
    "        y_pred.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    scheduler.step()\n",
    "    acc = correct / total\n",
    "    eer = compute_eer(y_true, y_score)\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return total_loss / total, acc, eer, auc, f1\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    y_true, y_score, y_pred = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            x = batch[\"input_values\"].to(device)\n",
    "            y = batch[\"labels\"].to(device)\n",
    "\n",
    "            out = model(input_values=x)  # ✅ AST input\n",
    "            logits = out.logits\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=1)[:, 1]\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_score.extend(probs.cpu().numpy())\n",
    "            y_pred.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    acc = correct / total\n",
    "    eer = compute_eer(y_true, y_score)\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return total_loss / total, acc, eer, auc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efaf21d-60b5-4748-b64f-71d4226e737c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5700/5700 [15:45<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.067523 | Train Acc: 0.980768 | Train EER: 0.020526 | Train AUC: 0.997945 | Train F1: 0.980893\n",
      "Model saved to /root/autodl-tmp/ast_ws_lp/epoch_16.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 3106/3106 [09:20<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev   Loss: 0.084035 | Dev   Acc: 0.967839 | Dev EER: 0.042542 | Dev AUC: 0.992590 | Dev F1: 0.856938\n",
      "\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5700/5700 [17:27<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.062891 | Train Acc: 0.981798 | Train EER: 0.017873 | Train AUC: 0.998304 | Train F1: 0.981857\n",
      "Model saved to /root/autodl-tmp/ast_ws_lp/epoch_17.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 3106/3106 [09:29<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev   Loss: 0.074224 | Dev   Acc: 0.971824 | Dev EER: 0.041236 | Dev AUC: 0.992836 | Dev F1: 0.870466\n",
      "\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5700/5700 [17:25<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.062177 | Train Acc: 0.982083 | Train EER: 0.017259 | Train AUC: 0.998260 | Train F1: 0.982330\n",
      "Model saved to /root/autodl-tmp/ast_ws_lp/epoch_18.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 3106/3106 [09:31<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev   Loss: 0.076568 | Dev   Acc: 0.970576 | Dev EER: 0.040440 | Dev AUC: 0.993001 | Dev F1: 0.866338\n",
      "\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5700/5700 [17:17<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.060431 | Train Acc: 0.983048 | Train EER: 0.016075 | Train AUC: 0.998370 | Train F1: 0.983317\n",
      "Model saved to /root/autodl-tmp/ast_ws_lp/epoch_19.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 3106/3106 [05:21<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev   Loss: 0.083999 | Dev   Acc: 0.967678 | Dev EER: 0.041146 | Dev AUC: 0.992900 | Dev F1: 0.856633\n",
      "\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5700/5700 [09:46<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.061695 | Train Acc: 0.981952 | Train EER: 0.017873 | Train AUC: 0.998300 | Train F1: 0.982169\n",
      "Model saved to /root/autodl-tmp/ast_ws_lp/epoch_20.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 3106/3106 [05:21<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev   Loss: 0.080307 | Dev   Acc: 0.968926 | Dev EER: 0.040748 | Dev AUC: 0.993125 | Dev F1: 0.861101\n"
     ]
    }
   ],
   "source": [
    "# training example\n",
    "model_path = \"/root/autodl-tmp/ast_ws_lp/epoch_15.pt\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "save_dir = \"/root/autodl-tmp/ast_ws_lp/\"\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(15, epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    train_loss, train_acc, train_eer, train_auc, train_f1 = train(model, train_dataloader)\n",
    "    print(f\"Train Loss: {train_loss:.6f} | Train Acc: {train_acc:.6f} | Train EER: {train_eer:.6f} | Train AUC: {train_auc:.6f} | Train F1: {train_f1:.6f}\" )\n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"epoch_{epoch+1}.pt\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "    dev_loss, dev_acc, dev_eer, dev_auc, dev_f1 = evaluate(model, dev_dataloader)\n",
    "    print(f\"Dev   Loss: {dev_loss:.6f} | Dev   Acc: {dev_acc:.6f} | Dev EER: {dev_eer:.6f} | Dev AUC: {dev_auc:.6f} | Dev F1: {dev_f1:.6f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
