# -*- coding: utf-8 -*-
"""Few-shot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cP64I2DFUSuhpXK_o-5Bms4szAePV-69

### **Training of AST and ConvNext-tiny with the ASVSpoof19 LA dataset applying Few-Shot, 20 samples in total (10 for bona fide and 10 for spoof).**
"""

# ============================
# Imports
# ============================
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import librosa
import numpy as np
from torch.utils.data import Dataset, DataLoader
from transformers import ASTModel, ASTConfig
import timm
from tqdm import tqdm

# ============================
# Few-Shot Dataset Loader
# ============================
class FewShotASVspoofDataset(Dataset):
    def __init__(self, protocol_file, audio_dir, model_type='ast',
                 sr=16000, n_mels=128, target_length=1024):
        self.df = pd.read_csv(protocol_file, sep=' ', header=None,
                              names=['speaker', 'file', 'system', 'env', 'label'])
        self.audio_dir = audio_dir
        self.model_type = model_type
        self.sr = sr
        self.n_mels = n_mels
        self.target_length = target_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        wav_path = os.path.join(self.audio_dir, row['file'] + '.flac')
        audio, _ = librosa.load(wav_path, sr=self.sr)
        mel = librosa.feature.melspectrogram(y=audio, sr=self.sr, n_mels=self.n_mels)
        logmel = librosa.power_to_db(mel)
        logmel = (logmel - logmel.mean()) / (logmel.std() + 1e-6)  # normalize

        if logmel.shape[1] < self.target_length:
            pad = self.target_length - logmel.shape[1]
            logmel = np.pad(logmel, ((0, 0), (0, pad)), mode='constant')
        else:
            logmel = logmel[:, :self.target_length]

        label = 1 if row['label'] == 'spoof' else 0

        if self.model_type == 'convnext':
            logmel = librosa.util.fix_length(logmel, size=224, axis=1)
            logmel = np.resize(logmel, (224, 224))
            logmel = np.stack([logmel] * 3, axis=0)  # 3-channel for ConvNeXt
        else:
            logmel = logmel[np.newaxis, :, :]  # 1-channel for AST: [1, 128, 1024]

        return torch.tensor(logmel, dtype=torch.float32), label

# ============================
# AST Model (Custom Classifier)
# ============================
def build_ast_model():
    config = ASTConfig.from_pretrained("MIT/ast-finetuned-audioset-10-10-0.4593")
    base_model = ASTModel.from_pretrained("MIT/ast-finetuned-audioset-10-10-0.4593", config=config)

    for param in base_model.parameters():
        param.requires_grad = False
    for param in base_model.encoder.layer[-1].parameters():
        param.requires_grad = True

    class ASTWithClassifier(nn.Module):
        def __init__(self, base_model, hidden_size=768, num_classes=2):
            super().__init__()
            self.base = base_model
            self.classifier = nn.Sequential(
                nn.LayerNorm(hidden_size),
                nn.Linear(hidden_size, num_classes)
            )

        def forward(self, x):
            # x shape: [B, 1, 128, 1024]
            x = x.squeeze(1)  # -> [B, 128, 1024]
            outputs = self.base(x)
            pooled = outputs.last_hidden_state.mean(dim=1)
            return self.classifier(pooled)

    return ASTWithClassifier(base_model)

# ============================
# ConvNeXt-Tiny Model
# ============================
def build_convnext_model():
    model = timm.create_model("convnext_tiny", pretrained=True, num_classes=2)

    for name, param in model.named_parameters():
        if "head" not in name:
            param.requires_grad = False
    for param in model.stages[-1].parameters():
        param.requires_grad = True

    return model

# ============================
# Training Loop
# ============================
def train_one_epoch(model, loader, optimizer, device):
    model.train()
    total_loss, correct = 0, 0
    for x, y in tqdm(loader, desc="Training", leave=False):
        x, y = x.to(device), y.to(device).long()
        outputs = model(x)
        loss = F.cross_entropy(outputs, y)
        preds = outputs.argmax(dim=1)
        correct += (preds == y).sum().item()
        total_loss += loss.item()
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    acc = correct / len(loader.dataset)
    print(f"Loss: {total_loss:.4f}, Accuracy: {acc:.4f}")

# ============================
# Run Training Function
# ============================
def run_training(model_type='ast'):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    protocol_path = "/content/sample_data/la_train/train_protocol.txt"
    audio_path = "/content/sample_data/la_train/train_fewshot"

    dataset = FewShotASVspoofDataset(protocol_path, audio_path, model_type=model_type)
    loader = DataLoader(dataset, batch_size=4, shuffle=True)

    model = build_ast_model() if model_type == 'ast' else build_convnext_model()
    model.to(device)
    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)

    for epoch in range(5):
        print(f"\nEpoch {epoch+1} - {model_type.upper()}")
        train_one_epoch(model, loader, optimizer, device)

    #  Save the model after training
    save_path = f"/content/drive/MyDrive/LA/models/{model_type}_finetuned.pth"
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    torch.save(model.state_dict(), save_path)
    print(f" Saved {model_type.upper()} model to: {save_path}")


# ============================
# Run Training for AST & ConvNeXt
# ============================
if __name__ == '__main__':
    run_training(model_type='ast')
    run_training(model_type='convnext')

"""# **Running the Development and evaluation set for AST and ConvNext-tiny**"""

import shutil
from google.colab import drive

# 1. Unmount if already mounted (does nothing if not)
drive.flush_and_unmount()

# 2. Delete the mount folder forcibly
shutil.rmtree("/content/drive", ignore_errors=True)

# ===============================
# 1. Mount Google Drive & Unzip
# ===============================
from google.colab import drive
import os

drive.mount('/content/drive')

os.makedirs("/content/data/dev", exist_ok=True)
os.makedirs("/content/data/eval", exist_ok=True)

# Force clean unzip with flattened structure
!unzip -o -j "/content/drive/MyDrive/LA/dev/flac_dev.zip" -d "/content/data/dev/flac_dev"
!unzip -o -j "/content/drive/MyDrive/LA/eval/flac_eval.zip" -d "/content/data/eval/flac_eval"

# ===============================
# 2. Imports and Dataset
# ===============================
import torch
import pandas as pd
import numpy as np
import librosa
from torch.utils.data import DataLoader, Dataset
from transformers import ASTModel, ASTConfig
import timm
from tqdm import tqdm

class FewShotASVspoofDataset(Dataset):
    def __init__(self, protocol_file, audio_dir, model_type='ast',
                 sr=16000, n_mels=128, target_length=1024):
        self.df = pd.read_csv(protocol_file, sep=' ', header=None,
                              names=['speaker', 'file', 'system', 'env', 'label'])
        self.audio_dir = audio_dir
        self.model_type = model_type
        self.sr = sr
        self.n_mels = n_mels
        self.target_length = target_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        wav_path = os.path.join(self.audio_dir, row['file'] + '.flac')
        audio, _ = librosa.load(wav_path, sr=self.sr)
        mel = librosa.feature.melspectrogram(y=audio, sr=self.sr, n_mels=self.n_mels)
        logmel = librosa.power_to_db(mel)
        logmel = (logmel - logmel.mean()) / (logmel.std() + 1e-6)

        if logmel.shape[1] < self.target_length:
            pad = self.target_length - logmel.shape[1]
            logmel = np.pad(logmel, ((0, 0), (0, pad)), mode='constant')
        else:
            logmel = logmel[:, :self.target_length]

        label = 1 if row['label'] == 'spoof' else 0

        if self.model_type == 'convnext':
            logmel = librosa.util.fix_length(logmel, size=224, axis=1)
            logmel = np.resize(logmel, (224, 224))
            logmel = np.stack([logmel] * 3, axis=0)
        else:
            logmel = logmel[np.newaxis, :, :]  # [1, 128, 1024]

        return torch.tensor(logmel, dtype=torch.float32), label

# ===============================
# 3. Models
# ===============================
def build_ast_model():
    config = ASTConfig.from_pretrained("MIT/ast-finetuned-audioset-10-10-0.4593")
    base_model = ASTModel.from_pretrained("MIT/ast-finetuned-audioset-10-10-0.4593", config=config)

    for param in base_model.parameters():
        param.requires_grad = False
    for param in base_model.encoder.layer[-1].parameters():
        param.requires_grad = True

    class ASTWithClassifier(torch.nn.Module):
        def __init__(self, base_model, hidden_size=768, num_classes=2):
            super().__init__()
            self.base = base_model
            self.classifier = torch.nn.Sequential(
                torch.nn.LayerNorm(hidden_size),
                torch.nn.Linear(hidden_size, num_classes)
            )

        def forward(self, x):
            x = x.squeeze(1)
            outputs = self.base(x)
            pooled = outputs.last_hidden_state.mean(dim=1)
            return self.classifier(pooled)

    return ASTWithClassifier(base_model)

def build_convnext_model():
    model = timm.create_model("convnext_tiny", pretrained=True, num_classes=2)
    for name, param in model.named_parameters():
        if "head" not in name:
            param.requires_grad = False
    for param in model.stages[-1].parameters():
        param.requires_grad = True
    return model

# ===============================
# 4. Inference Functions (with label)
# ===============================
def run_inference(model, protocol_file, audio_dir, model_type, device):
    model.eval()
    dataset = FewShotASVspoofDataset(protocol_file, audio_dir, model_type=model_type)
    loader = DataLoader(dataset, batch_size=1)

    scores = []
    with torch.no_grad():
        for i, (x, _) in enumerate(tqdm(loader, desc=f"Inference [{model_type}]")):
            x = x.to(device)
            output = model(x)
            score = torch.softmax(output, dim=1)[0, 1].item()
            row = dataset.df.iloc[i]
            file_id = row['file']
            label = row['label']
            scores.append((file_id, label, score))

    return scores

def save_scores(scores, output_path):
    with open(output_path, 'w') as f:
        for file_id, label, score in scores:
            f.write(f"{file_id} {label} {score:.6f}\n")

# ===============================
# 5. Evaluate Function
# ===============================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def evaluate_model(model_type, model_path):
    print(f"\nEvaluating {model_type.upper()}")

    model = build_ast_model() if model_type == 'ast' else build_convnext_model()
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)

    dev_audio_dir = "/content/data/dev/flac_dev"
    eval_audio_dir = "/content/data/eval/flac_eval"

    dev_protocol = "/content/sample_data/la_train/ASVspoof2019.LA.cm.dev.trl.txt"
    eval_protocol = "/content/sample_data/la_train/ASVspoof2019.LA.cm.eval.trl.txt"

    dev_scores = run_inference(model, dev_protocol, dev_audio_dir, model_type, device)
    dev_score_path = f"cm_scores_dev_{model_type}.txt"
    save_scores(dev_scores, dev_score_path)

    eval_scores = run_inference(model, eval_protocol, eval_audio_dir, model_type, device)
    eval_score_path = f"cm_scores_eval_{model_type}.txt"
    save_scores(eval_scores, eval_score_path)

    print(f"Saved:\n  {dev_score_path}\n  {eval_score_path}")

# ===============================
# 6. Run Evaluation
# ===============================
evaluate_model("ast", "/content/drive/MyDrive/LA/models/ast_finetuned.pth")
evaluate_model("convnext", "/content/drive/MyDrive/LA/models/convnext_finetuned.pth")

"""# **Calculating EER and t-DCF for AST Development set**"""

import numpy as np
import matplotlib.pyplot as plt
import eval_metrics as em  # Make sure eval_metrics.py is uploaded

# =============================
# ✅ Set CM & ASV score file paths (DEV)
# =============================
cm_score_file = '/content/cm_scores_dev_ast.txt'      # CM scores: 3-column <utt> <label> <score>
asv_score_file = '/content/asv_dev.txt'               # ASV scores: 3-column <utt> <label> <score>

# =============================
# ✅ t-DCF Cost Parameters
# =============================
Pspoof = 0.05
cost_model = {
    'Pspoof': Pspoof,
    'Ptar': (1 - Pspoof) * 0.99,
    'Pnon': (1 - Pspoof) * 0.01,
    'Cmiss_asv': 1,
    'Cfa_asv': 10,
    'Cmiss_cm': 1,
    'Cfa_cm': 10,
}

# =============================
# ✅ Load ASV scores (3-column format)
# =============================
asv_data = np.genfromtxt(asv_score_file, dtype=str)
asv_keys = asv_data[:, 1]                              # 'target', 'nontarget', 'spoof'
asv_scores = asv_data[:, 2].astype(np.float32)

# =============================
# ✅ Load CM scores (3-column format)
# =============================
cm_data = np.genfromtxt(cm_score_file, dtype=str)
cm_keys = cm_data[:, 1]                                # 'bonafide', 'spoof'
cm_scores = cm_data[:, 2].astype(np.float32)

# =============================
# ✅ Split scores into groups
# =============================
tar_asv   = asv_scores[asv_keys == 'target']
non_asv   = asv_scores[asv_keys == 'nontarget']
spoof_asv = asv_scores[asv_keys == 'spoof']

bona_cm   = cm_scores[cm_keys == 'bonafide']
spoof_cm  = cm_scores[cm_keys == 'spoof']

# =============================
# ✅ Compute EERs + Thresholds
# =============================
eer_asv, asv_threshold = em.compute_eer(tar_asv, non_asv)
eer_cm = em.compute_eer(bona_cm, spoof_cm)[0]

Pfa_asv, Pmiss_asv, Pmiss_spoof_asv = em.obtain_asv_error_rates(
    tar_asv, non_asv, spoof_asv, asv_threshold
)

# =============================
# ✅ Compute t-DCF
# =============================
tDCF_curve, CM_thresholds = em.compute_tDCF(
    bona_cm, spoof_cm,
    Pfa_asv, Pmiss_asv, Pmiss_spoof_asv,
    cost_model, True
)

min_tDCF_index = np.argmin(tDCF_curve)
min_tDCF = tDCF_curve[min_tDCF_index]

# =============================
# ✅ Print Results
# =============================
print('ASV SYSTEM')
print('   EER            = {:8.5f} %'.format(eer_asv * 100))
print('   Pfa            = {:8.5f} %'.format(Pfa_asv * 100))
print('   Pmiss          = {:8.5f} %'.format(Pmiss_asv * 100))
print('   1-Pmiss,spoof  = {:8.5f} %'.format((1 - Pmiss_spoof_asv) * 100))

print('\nCM SYSTEM')
print('   EER            = {:8.5f} %'.format(eer_cm * 100))

print('\nTANDEM')
print('   min-tDCF       = {:8.5f}'.format(min_tDCF))

# =============================
# ✅ Plots
# =============================
plt.figure(figsize=(12, 5))
plt.subplot(121)
plt.hist(tar_asv, bins=50, density=True, histtype='step', label='Target')
plt.hist(non_asv, bins=50, density=True, histtype='step', label='Nontarget')
plt.hist(spoof_asv, bins=50, density=True, histtype='step', label='Spoof')
plt.axvline(asv_threshold, color='black', linestyle='dotted', label='EER threshold')
plt.title("ASV Score Histogram")
plt.xlabel("ASV Score")
plt.ylabel("Density")
plt.legend()

plt.subplot(122)
plt.hist(bona_cm, bins=50, density=True, histtype='step', label='Bona fide')
plt.hist(spoof_cm, bins=50, density=True, histtype='step', label='Spoof')
plt.title("CM Score Histogram")
plt.xlabel("CM Score")
plt.ylabel("Density")
plt.legend()

plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 4))
plt.plot(CM_thresholds, tDCF_curve, label='t-DCF')
plt.plot(CM_thresholds[min_tDCF_index], min_tDCF, 'ro', label=f'min t-DCF = {min_tDCF:.5f}')
plt.axhline(1.0, linestyle='--', color='gray', label='t-DCF = 1 (bad CM)')
plt.title('Normalized t-DCF Curve')
plt.xlabel('CM threshold index')
plt.ylabel('Normalized t-DCF')
plt.ylim([0, 1.5])
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""# **Calculating EER and t-DCF for AST Evaluation set**"""

import numpy as np
import matplotlib.pyplot as plt
import eval_metrics as em  # Ensure eval_metrics.py is uploaded to /content

# =============================
# ✅ Set CM & ASV score file paths
# =============================
cm_score_file = '/content/cm_scores_eval_ast.txt'  # 3-column: <utt> <label> <score>
asv_score_file = '/content/ASVspoof2019.LA.asv.eval.gi.trl.scores.txt'  # 3-column official ASV file


# =============================
# ✅ t-DCF Cost Parameters
# =============================
Pspoof = 0.05
cost_model = {
    'Pspoof': Pspoof,
    'Ptar': (1 - Pspoof) * 0.99,
    'Pnon': (1 - Pspoof) * 0.01,
    'Cmiss_asv': 1,
    'Cfa_asv': 10,
    'Cmiss_cm': 1,
    'Cfa_cm': 10,
}

# =============================
# ✅ Load ASV scores (3-column format)
# =============================
asv_data = np.genfromtxt(asv_score_file, dtype=str)
asv_sources = asv_data[:, 0]
asv_keys = asv_data[:, 1]
asv_scores = asv_data[:, 2].astype(np.float32)

# =============================
# ✅ Load CM scores (3-column format)
# =============================
cm_data = np.genfromtxt(cm_score_file, dtype=str)
cm_utt_id = cm_data[:, 0]
cm_keys = cm_data[:, 1]                # 'spoof' or 'bonafide'
cm_scores = cm_data[:, 2].astype(np.float32)

# =============================
# ✅ Split scores into groups
# =============================
tar_asv   = asv_scores[asv_keys == 'target']
non_asv   = asv_scores[asv_keys == 'nontarget']
spoof_asv = asv_scores[asv_keys == 'spoof']

bona_cm   = cm_scores[cm_keys == 'bonafide']
spoof_cm  = cm_scores[cm_keys == 'spoof']

# =============================
# ✅ Compute EERs + Thresholds
# =============================
eer_asv, asv_threshold = em.compute_eer(tar_asv, non_asv)
eer_cm = em.compute_eer(bona_cm, spoof_cm)[0]

Pfa_asv, Pmiss_asv, Pmiss_spoof_asv = em.obtain_asv_error_rates(
    tar_asv, non_asv, spoof_asv, asv_threshold
)

# =============================
# ✅ Compute t-DCF
# =============================
tDCF_curve, CM_thresholds = em.compute_tDCF(
    bona_cm, spoof_cm,
    Pfa_asv, Pmiss_asv, Pmiss_spoof_asv,
    cost_model, True
)

min_tDCF_index = np.argmin(tDCF_curve)
min_tDCF = tDCF_curve[min_tDCF_index]

# =============================
# ✅ Print Results
# =============================
print('ASV SYSTEM')
print('   EER            = {:8.5f} %'.format(eer_asv * 100))
print('   Pfa            = {:8.5f} %'.format(Pfa_asv * 100))
print('   Pmiss          = {:8.5f} %'.format(Pmiss_asv * 100))
print('   1-Pmiss,spoof  = {:8.5f} %'.format((1 - Pmiss_spoof_asv) * 100))

print('\nCM SYSTEM')
print('   EER            = {:8.5f} %'.format(eer_cm * 100))

print('\nTANDEM')
print('   min-tDCF       = {:8.5f}'.format(min_tDCF))

# =============================
# ✅ Plots
# =============================
plt.figure(figsize=(12, 5))
plt.subplot(121)
plt.hist(tar_asv, bins=50, density=True, histtype='step', label='Target')
plt.hist(non_asv, bins=50, density=True, histtype='step', label='Nontarget')
plt.hist(spoof_asv, bins=50, density=True, histtype='step', label='Spoof')
plt.axvline(asv_threshold, color='black', linestyle='dotted', label='EER threshold')
plt.title("ASV Score Histogram")
plt.xlabel("ASV Score")
plt.ylabel("Density")
plt.legend()

plt.subplot(122)
plt.hist(bona_cm, bins=50, density=True, histtype='step', label='Bona fide')
plt.hist(spoof_cm, bins=50, density=True, histtype='step', label='Spoof')
plt.title("CM Score Histogram")
plt.xlabel("CM Score")
plt.ylabel("Density")
plt.legend()

plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 4))
plt.plot(CM_thresholds, tDCF_curve, label='t-DCF')
plt.plot(CM_thresholds[min_tDCF_index], min_tDCF, 'ro', label=f'min t-DCF = {min_tDCF:.5f}')
plt.axhline(1.0, linestyle='--', color='gray', label='t-DCF = 1 (bad CM)')
plt.title('Normalized t-DCF Curve')
plt.xlabel('CM threshold index')
plt.ylabel('Normalized t-DCF')
plt.ylim([0, 1.5])
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""# **Calculating EER and t-DCF for ConvNext-tiny Development set**"""

import numpy as np
import matplotlib.pyplot as plt
import eval_metrics as em  # Make sure eval_metrics.py is uploaded

# =============================
# ✅ Set CM & ASV score file paths (DEV)
# =============================
cm_score_file = '/content/cm_scores_dev_convnext (2).txt'      # CM scores: 3-column <utt> <label> <score>
asv_score_file = '/content/asv_dev.txt'               # ASV scores: 3-column <utt> <label> <score>

# =============================
# ✅ t-DCF Cost Parameters
# =============================
Pspoof = 0.05
cost_model = {
    'Pspoof': Pspoof,
    'Ptar': (1 - Pspoof) * 0.99,
    'Pnon': (1 - Pspoof) * 0.01,
    'Cmiss_asv': 1,
    'Cfa_asv': 10,
    'Cmiss_cm': 1,
    'Cfa_cm': 10,
}

# =============================
# ✅ Load ASV scores (3-column format)
# =============================
asv_data = np.genfromtxt(asv_score_file, dtype=str)
asv_keys = asv_data[:, 1]                              # 'target', 'nontarget', 'spoof'
asv_scores = asv_data[:, 2].astype(np.float32)

# =============================
# ✅ Load CM scores (3-column format)
# =============================
cm_data = np.genfromtxt(cm_score_file, dtype=str)
cm_keys = cm_data[:, 1]                                # 'bonafide', 'spoof'
cm_scores = cm_data[:, 2].astype(np.float32)

# =============================
# ✅ Split scores into groups
# =============================
tar_asv   = asv_scores[asv_keys == 'target']
non_asv   = asv_scores[asv_keys == 'nontarget']
spoof_asv = asv_scores[asv_keys == 'spoof']

bona_cm   = cm_scores[cm_keys == 'bonafide']
spoof_cm  = cm_scores[cm_keys == 'spoof']

# =============================
# ✅ Compute EERs + Thresholds
# =============================
eer_asv, asv_threshold = em.compute_eer(tar_asv, non_asv)
eer_cm = em.compute_eer(bona_cm, spoof_cm)[0]

Pfa_asv, Pmiss_asv, Pmiss_spoof_asv = em.obtain_asv_error_rates(
    tar_asv, non_asv, spoof_asv, asv_threshold
)

# =============================
# ✅ Compute t-DCF
# =============================
tDCF_curve, CM_thresholds = em.compute_tDCF(
    bona_cm, spoof_cm,
    Pfa_asv, Pmiss_asv, Pmiss_spoof_asv,
    cost_model, True
)

min_tDCF_index = np.argmin(tDCF_curve)
min_tDCF = tDCF_curve[min_tDCF_index]

# =============================
# ✅ Print Results
# =============================
print('ASV SYSTEM')
print('   EER            = {:8.5f} %'.format(eer_asv * 100))
print('   Pfa            = {:8.5f} %'.format(Pfa_asv * 100))
print('   Pmiss          = {:8.5f} %'.format(Pmiss_asv * 100))
print('   1-Pmiss,spoof  = {:8.5f} %'.format((1 - Pmiss_spoof_asv) * 100))

print('\nCM SYSTEM')
print('   EER            = {:8.5f} %'.format(eer_cm * 100))

print('\nTANDEM')
print('   min-tDCF       = {:8.5f}'.format(min_tDCF))

# =============================
# ✅ Plots
# =============================
plt.figure(figsize=(12, 5))
plt.subplot(121)
plt.hist(tar_asv, bins=50, density=True, histtype='step', label='Target')
plt.hist(non_asv, bins=50, density=True, histtype='step', label='Nontarget')
plt.hist(spoof_asv, bins=50, density=True, histtype='step', label='Spoof')
plt.axvline(asv_threshold, color='black', linestyle='dotted', label='EER threshold')
plt.title("ASV Score Histogram")
plt.xlabel("ASV Score")
plt.ylabel("Density")
plt.legend()

plt.subplot(122)
plt.hist(bona_cm, bins=50, density=True, histtype='step', label='Bona fide')
plt.hist(spoof_cm, bins=50, density=True, histtype='step', label='Spoof')
plt.title("CM Score Histogram")
plt.xlabel("CM Score")
plt.ylabel("Density")
plt.legend()

plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 4))
plt.plot(CM_thresholds, tDCF_curve, label='t-DCF')
plt.plot(CM_thresholds[min_tDCF_index], min_tDCF, 'ro', label=f'min t-DCF = {min_tDCF:.5f}')
plt.axhline(1.0, linestyle='--', color='gray', label='t-DCF = 1 (bad CM)')
plt.title('Normalized t-DCF Curve')
plt.xlabel('CM threshold index')
plt.ylabel('Normalized t-DCF')
plt.ylim([0, 1.5])
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""# **Calculating EER and t-DCF for ConvNext-tiny Evaluation set**"""

import numpy as np
import matplotlib.pyplot as plt
import eval_metrics as em  # Ensure eval_metrics.py is uploaded to /content

# =============================
# ✅ Set CM & ASV score file paths
# =============================
cm_score_file = '/content/cm_scores_eval_convnext.txt'  # 3-column: <utt> <label> <score>
asv_score_file = '/content/ASVspoof2019.LA.asv.eval.gi.trl.scores.txt'  # 3-column official ASV file


# =============================
# ✅ t-DCF Cost Parameters
# =============================
Pspoof = 0.05
cost_model = {
    'Pspoof': Pspoof,
    'Ptar': (1 - Pspoof) * 0.99,
    'Pnon': (1 - Pspoof) * 0.01,
    'Cmiss_asv': 1,
    'Cfa_asv': 10,
    'Cmiss_cm': 1,
    'Cfa_cm': 10,
}

# =============================
# ✅ Load ASV scores (3-column format)
# =============================
asv_data = np.genfromtxt(asv_score_file, dtype=str)
asv_sources = asv_data[:, 0]
asv_keys = asv_data[:, 1]
asv_scores = asv_data[:, 2].astype(np.float32)

# =============================
# ✅ Load CM scores (3-column format)
# =============================
cm_data = np.genfromtxt(cm_score_file, dtype=str)
cm_utt_id = cm_data[:, 0]
cm_keys = cm_data[:, 1]                # 'spoof' or 'bonafide'
cm_scores = cm_data[:, 2].astype(np.float32)

# =============================
# ✅ Split scores into groups
# =============================
tar_asv   = asv_scores[asv_keys == 'target']
non_asv   = asv_scores[asv_keys == 'nontarget']
spoof_asv = asv_scores[asv_keys == 'spoof']

bona_cm   = cm_scores[cm_keys == 'bonafide']
spoof_cm  = cm_scores[cm_keys == 'spoof']

# =============================
# ✅ Compute EERs + Thresholds
# =============================
eer_asv, asv_threshold = em.compute_eer(tar_asv, non_asv)
eer_cm = em.compute_eer(bona_cm, spoof_cm)[0]

Pfa_asv, Pmiss_asv, Pmiss_spoof_asv = em.obtain_asv_error_rates(
    tar_asv, non_asv, spoof_asv, asv_threshold
)

# =============================
# ✅ Compute t-DCF
# =============================
tDCF_curve, CM_thresholds = em.compute_tDCF(
    bona_cm, spoof_cm,
    Pfa_asv, Pmiss_asv, Pmiss_spoof_asv,
    cost_model, True
)

min_tDCF_index = np.argmin(tDCF_curve)
min_tDCF = tDCF_curve[min_tDCF_index]

# =============================
# ✅ Print Results
# =============================
print('ASV SYSTEM')
print('   EER            = {:8.5f} %'.format(eer_asv * 100))
print('   Pfa            = {:8.5f} %'.format(Pfa_asv * 100))
print('   Pmiss          = {:8.5f} %'.format(Pmiss_asv * 100))
print('   1-Pmiss,spoof  = {:8.5f} %'.format((1 - Pmiss_spoof_asv) * 100))

print('\nCM SYSTEM')
print('   EER            = {:8.5f} %'.format(eer_cm * 100))

print('\nTANDEM')
print('   min-tDCF       = {:8.5f}'.format(min_tDCF))

# =============================
# ✅ Plots
# =============================
plt.figure(figsize=(12, 5))
plt.subplot(121)
plt.hist(tar_asv, bins=50, density=True, histtype='step', label='Target')
plt.hist(non_asv, bins=50, density=True, histtype='step', label='Nontarget')
plt.hist(spoof_asv, bins=50, density=True, histtype='step', label='Spoof')
plt.axvline(asv_threshold, color='black', linestyle='dotted', label='EER threshold')
plt.title("ASV Score Histogram")
plt.xlabel("ASV Score")
plt.ylabel("Density")
plt.legend()

plt.subplot(122)
plt.hist(bona_cm, bins=50, density=True, histtype='step', label='Bona fide')
plt.hist(spoof_cm, bins=50, density=True, histtype='step', label='Spoof')
plt.title("CM Score Histogram")
plt.xlabel("CM Score")
plt.ylabel("Density")
plt.legend()

plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 4))
plt.plot(CM_thresholds, tDCF_curve, label='t-DCF')
plt.plot(CM_thresholds[min_tDCF_index], min_tDCF, 'ro', label=f'min t-DCF = {min_tDCF:.5f}')
plt.axhline(1.0, linestyle='--', color='gray', label='t-DCF = 1 (bad CM)')
plt.title('Normalized t-DCF Curve')
plt.xlabel('CM threshold index')
plt.ylabel('Normalized t-DCF')
plt.ylim([0, 1.5])
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()