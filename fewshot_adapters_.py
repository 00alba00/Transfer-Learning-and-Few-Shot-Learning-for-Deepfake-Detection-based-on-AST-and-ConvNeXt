# -*- coding: utf-8 -*-
"""Fewshot_Adapters.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IerfcfoUHi8gVdMKDB2eFSlh0PF3RO_Q

# **Training of AST and ConvNext-tiny with the ASVSpoof19 LA dataset applying Few-Shot, 20 samples in total (10 for bona fide and 10 for spoof), and an Adapter to the last 2 layers of AST and to every block in the last stage of ConvNext-tiny**
"""

# ============================
# Imports
# ============================
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import librosa
import numpy as np
from torch.utils.data import Dataset, DataLoader
from transformers import ASTModel, ASTConfig
import timm
from tqdm import tqdm

# ============================
# Few-Shot Dataset Loader
# ============================
class FewShotASVspoofDataset(Dataset):
    def __init__(self, protocol_file, audio_dir, model_type='ast',
                 sr=16000, n_mels=128, target_length=1024):
        self.df = pd.read_csv(protocol_file, sep=' ', header=None,
                              names=['speaker', 'file', 'system', 'env', 'label'])
        self.audio_dir = audio_dir
        self.model_type = model_type
        self.sr = sr
        self.n_mels = n_mels
        self.target_length = target_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        wav_path = os.path.join(self.audio_dir, row['file'] + '.flac')
        audio, _ = librosa.load(wav_path, sr=self.sr)
        mel = librosa.feature.melspectrogram(y=audio, sr=self.sr, n_mels=self.n_mels)
        logmel = librosa.power_to_db(mel)
        logmel = (logmel - logmel.mean()) / (logmel.std() + 1e-6)

        if logmel.shape[1] < self.target_length:
            pad = self.target_length - logmel.shape[1]
            logmel = np.pad(logmel, ((0, 0), (0, pad)), mode='constant')
        else:
            logmel = logmel[:, :self.target_length]

        label = 1 if row['label'] == 'spoof' else 0

        if self.model_type == 'convnext':
            logmel = librosa.util.fix_length(logmel, size=224, axis=1)
            logmel = np.resize(logmel, (224, 224))
            logmel = np.stack([logmel] * 3, axis=0)
        else:
            logmel = logmel[np.newaxis, :, :]

        return torch.tensor(logmel, dtype=torch.float32), label

# ============================
# Adapter Modules
# ============================
class Adapter(nn.Module):
    def __init__(self, input_dim, bottleneck_dim=64):
        super().__init__()
        self.adapter = nn.Sequential(
            nn.Linear(input_dim, bottleneck_dim),
            nn.ReLU(),
            nn.Linear(bottleneck_dim, input_dim)
        )

    def forward(self, x):
        return x + self.adapter(x)

class ConvAdapter(nn.Module):
    def __init__(self, channels, reduction=4):
        super().__init__()
        self.adapter = nn.Sequential(
            nn.Conv2d(channels, channels // reduction, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // reduction, channels, kernel_size=1)
        )

    def forward(self, x):
        return x + self.adapter(x)

# ============================
# AST Model with Adapters
# ============================
def build_ast_model_with_adapters():
    config = ASTConfig.from_pretrained("MIT/ast-finetuned-audioset-10-10-0.4593")
    base_model = ASTModel.from_pretrained("MIT/ast-finetuned-audioset-10-10-0.4593", config=config)

    for param in base_model.parameters():
        param.requires_grad = False

    num_layers = len(base_model.encoder.layer)
    for i in range(num_layers - 2, num_layers):
        layer = base_model.encoder.layer[i]
        adapter = Adapter(input_dim=config.hidden_size)
        layer.adapter = adapter

        orig_forward = layer.forward

        def forward_with_adapter(self, hidden_states, *args, **kwargs):
            output = orig_forward(hidden_states, *args, **kwargs)[0]
            return (self.adapter(output),)

        layer.forward = forward_with_adapter.__get__(layer, layer.__class__)

    class ASTWithAdapterClassifier(nn.Module):
        def __init__(self, base_model, hidden_size=768, num_classes=2):
            super().__init__()
            self.base = base_model
            self.classifier = nn.Sequential(
                nn.LayerNorm(hidden_size),
                nn.Linear(hidden_size, num_classes)
            )

        def forward(self, x):
            x = x.squeeze(1)
            outputs = self.base(x)
            pooled = outputs.last_hidden_state.mean(dim=1)
            return self.classifier(pooled)

    return ASTWithAdapterClassifier(base_model)

# ============================
# ConvNeXt Model with Adapters
# ============================
def build_convnext_with_adapters():
    model = timm.create_model("convnext_tiny", pretrained=True, num_classes=2)

    for name, param in model.named_parameters():
        if "head" not in name:
            param.requires_grad = False

    last_stage = model.stages[-1]
    if not hasattr(last_stage, "blocks"):
        raise TypeError("Expected last stage to have 'blocks' attribute.")

    blocks = last_stage.blocks
    for i, block in enumerate(blocks):
        if hasattr(block, 'dwconv'):
            channels = block.dwconv.out_channels
        elif hasattr(block, 'conv_dw'):
            channels = block.conv_dw.out_channels
        else:
            raise AttributeError("Cannot find depthwise conv layer.")

        block.adapter = ConvAdapter(channels=channels)
        orig_forward = block.forward

        def forward_with_adapter(self, x):
            x = orig_forward(x)
            return self.adapter(x)

        block.forward = forward_with_adapter.__get__(block, block.__class__)

        for param in block.adapter.parameters():
            param.requires_grad = True

    return model

# ============================
# Training Loop
# ============================
def train_one_epoch(model, loader, optimizer, device):
    model.train()
    total_loss, correct = 0, 0
    for x, y in tqdm(loader, desc="Training", leave=False):
        x, y = x.to(device), y.to(device).long()
        outputs = model(x)
        loss = F.cross_entropy(outputs, y)
        preds = outputs.argmax(dim=1)
        correct += (preds == y).sum().item()
        total_loss += loss.item()
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    acc = correct / len(loader.dataset)
    print(f"Loss: {total_loss:.4f}, Accuracy: {acc:.4f}")

# ============================
# Run Training Function
# ============================
def run_training(model_type='ast'):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    protocol_path = "/content/sample_data/la_train/train_protocol.txt"
    audio_path = "/content/sample_data/la_train/train_fewshot"

    dataset = FewShotASVspoofDataset(protocol_path, audio_path, model_type=model_type)
    loader = DataLoader(dataset, batch_size=4, shuffle=True)

    model = build_ast_model_with_adapters() if model_type == 'ast' else build_convnext_with_adapters()
    model.to(device)

    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)

    for epoch in range(5):
        print(f"\nEpoch {epoch+1} - {model_type.upper()}")
        train_one_epoch(model, loader, optimizer, device)

    # ðŸ” Save trained adapter-based model
    save_path = f"/content/drive/MyDrive/LA/models/{model_type}_adapter.pth"
    torch.save(model.state_dict(), save_path)
    print(f"âœ… Saved {model_type.upper()} model with adapters to: {save_path}")

# ============================
# Run Training for AST & ConvNeXt
# ============================
if __name__ == '__main__':
    run_training(model_type='ast')
    run_training(model_type='convnext')

"""# **Running the Development and evaluation set for AST and ConvNext-tiny**"""

import shutil
from google.colab import drive

# 1. Unmount if already mounted (does nothing if not)
drive.flush_and_unmount()

# 2. Delete the mount folder forcibly
shutil.rmtree("/content/drive", ignore_errors=True)

# ====================================
# 1. Mount & Unzip (unchanged)
# ====================================
from google.colab import drive
import os

drive.mount('/content/drive')

os.makedirs("/content/data/dev", exist_ok=True)
os.makedirs("/content/data/eval", exist_ok=True)

!unzip -o -j "/content/drive/MyDrive/LA/dev/flac_dev.zip" -d "/content/data/dev/flac_dev"
!unzip -o -j "/content/drive/MyDrive/LA/eval/flac_eval.zip" -d "/content/data/eval/flac_eval"

# ====================================
# 2. Imports
# ====================================
import torch
import pandas as pd
import numpy as np
import librosa
from torch.utils.data import DataLoader, Dataset
from transformers import ASTModel, ASTConfig
import timm
from tqdm import tqdm
from torch import nn
import torch.nn.functional as F

# ====================================
# Dataset
# ====================================
class FewShotASVspoofDataset(Dataset):
    def __init__(self, protocol_file, audio_dir, model_type='ast',
                 sr=16000, n_mels=128, target_length=1024):
        self.df = pd.read_csv(protocol_file, sep=' ', header=None,
                              names=['speaker', 'file', 'system', 'env', 'label'])
        self.audio_dir = audio_dir
        self.model_type = model_type
        self.sr = sr
        self.n_mels = n_mels
        self.target_length = target_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        wav_path = os.path.join(self.audio_dir, row['file'] + '.flac')
        audio, _ = librosa.load(wav_path, sr=self.sr)
        mel = librosa.feature.melspectrogram(y=audio, sr=self.sr, n_mels=self.n_mels)
        logmel = librosa.power_to_db(mel)
        logmel = (logmel - logmel.mean()) / (logmel.std() + 1e-6)

        if logmel.shape[1] < self.target_length:
            pad = self.target_length - logmel.shape[1]
            logmel = np.pad(logmel, ((0, 0), (0, pad)), mode='constant')
        else:
            logmel = logmel[:, :self.target_length]

        label = 1 if row['label'] == 'spoof' else 0

        if self.model_type == 'convnext':
            logmel = librosa.util.fix_length(logmel, size=224, axis=1)
            logmel = np.resize(logmel, (224, 224))
            logmel = np.stack([logmel] * 3, axis=0)
        else:
            logmel = logmel[np.newaxis, :, :]

        return torch.tensor(logmel, dtype=torch.float32), label

# ====================================
# Adapters
# ====================================
class Adapter(nn.Module):
    def __init__(self, input_dim, bottleneck_dim=64):
        super().__init__()
        self.adapter = nn.Sequential(
            nn.Linear(input_dim, bottleneck_dim),
            nn.ReLU(),
            nn.Linear(bottleneck_dim, input_dim)
        )
    def forward(self, x):
        return x + self.adapter(x)

class ConvAdapter(nn.Module):
    def __init__(self, channels, reduction=4):
        super().__init__()
        self.adapter = nn.Sequential(
            nn.Conv2d(channels, channels // reduction, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // reduction, channels, kernel_size=1)
        )
    def forward(self, x):
        return x + self.adapter(x)

# ====================================
# AST with Adapters
# ====================================
def build_ast_model_with_adapters():
    config = ASTConfig.from_pretrained("MIT/ast-finetuned-audioset-10-10-0.4593")
    base_model = ASTModel.from_pretrained("MIT/ast-finetuned-audioset-10-10-0.4593", config=config)
    for param in base_model.parameters():
        param.requires_grad = False

    num_layers = len(base_model.encoder.layer)
    for i in range(num_layers - 2, num_layers):
        layer = base_model.encoder.layer[i]
        adapter = Adapter(input_dim=config.hidden_size)
        layer.adapter = adapter
        orig_forward = layer.forward
        def forward_with_adapter(self, hidden_states, *args, **kwargs):
            output = orig_forward(hidden_states, *args, **kwargs)[0]
            return (self.adapter(output),)
        layer.forward = forward_with_adapter.__get__(layer, layer.__class__)

    class ASTWithAdapterClassifier(nn.Module):
        def __init__(self, base_model, hidden_size=768, num_classes=2):
            super().__init__()
            self.base = base_model
            self.classifier = nn.Sequential(
                nn.LayerNorm(hidden_size),
                nn.Linear(hidden_size, num_classes)
            )

        def forward(self, x):
            x = x.squeeze(1)
            outputs = self.base(x)
            pooled = outputs.last_hidden_state.mean(dim=1)
            return self.classifier(pooled)

    return ASTWithAdapterClassifier(base_model)

# ====================================
# ConvNeXt with Adapters
# ====================================
def build_convnext_with_adapters():
    model = timm.create_model("convnext_tiny", pretrained=True, num_classes=2)
    for name, param in model.named_parameters():
        if "head" not in name:
            param.requires_grad = False

    last_stage = model.stages[-1]
    blocks = last_stage.blocks if hasattr(last_stage, "blocks") else None
    if blocks is None:
        raise TypeError("No blocks found in ConvNeXt's last stage.")

    for i, block in enumerate(blocks):
        if hasattr(block, 'dwconv'):
            channels = block.dwconv.out_channels
        elif hasattr(block, 'conv_dw'):
            channels = block.conv_dw.out_channels
        else:
            raise AttributeError("Cannot find depthwise conv layer.")

        block.adapter = ConvAdapter(channels=channels)
        orig_forward = block.forward

        def forward_with_adapter(self, x):
            x = orig_forward(x)
            return self.adapter(x)

        block.forward = forward_with_adapter.__get__(block, block.__class__)
        for param in block.adapter.parameters():
            param.requires_grad = True

    return model

# ====================================
# Inference
# ====================================
def run_inference(model, protocol_file, audio_dir, model_type, device):
    model.eval()
    dataset = FewShotASVspoofDataset(protocol_file, audio_dir, model_type=model_type)
    loader = DataLoader(dataset, batch_size=1)

    scores = []
    with torch.no_grad():
        for i, (x, _) in enumerate(tqdm(loader, desc=f"Inference [{model_type}]")):
            x = x.to(device)
            output = model(x)
            score = torch.softmax(output, dim=1)[0, 1].item()
            row = dataset.df.iloc[i]
            file_id = row['file']
            label = row['label']
            scores.append((file_id, label, score))
    return scores

def save_scores(scores, output_path):
    with open(output_path, 'w') as f:
        for file_id, label, score in scores:
            f.write(f"{file_id} {label} {score:.6f}\n")

# ====================================
# Evaluation Wrapper
# ====================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def evaluate_model(model_type, model_path):
    print(f"\nEvaluating {model_type.upper()}")

    if model_type == 'ast':
        model = build_ast_model_with_adapters()
    else:
        model = build_convnext_with_adapters()

    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)

    dev_audio_dir = "/content/data/dev/flac_dev"
    eval_audio_dir = "/content/data/eval/flac_eval"

    dev_protocol = "/content/sample_data/la_train/ASVspoof2019.LA.cm.dev.trl.txt"
    eval_protocol = "/content/sample_data/la_train/ASVspoof2019.LA.cm.eval.trl.txt"

    dev_scores = run_inference(model, dev_protocol, dev_audio_dir, model_type, device)
    save_scores(dev_scores, f"cm_scores_dev_{model_type}.txt")

    eval_scores = run_inference(model, eval_protocol, eval_audio_dir, model_type, device)
    save_scores(eval_scores, f"cm_scores_eval_{model_type}.txt")

    print(f"âœ… Saved:\n  cm_scores_dev_{model_type}.txt\n  cm_scores_eval_{model_type}.txt")

# ====================================
# Run Evaluation on Adapter-Based Models
# ====================================
evaluate_model("ast", "/content/drive/MyDrive/LA/models/ast_adapter.pth")
evaluate_model("convnext", "/content/drive/MyDrive/LA/models/convnext_adapter.pth")

"""# **Calculating EER and t-DCF for AST Development set**"""

import numpy as np
import matplotlib.pyplot as plt
import eval_metrics as em

# =============================
# Set CM & ASV score file paths (DEV)
# =============================
cm_score_file = '/content/cm_scores_dev_ast (1).txt'      # CM scores: 3-column <utt> <label> <score>
asv_score_file = '/content/asv_dev.txt'               # ASV scores: 3-column <utt> <label> <score>

# =============================
# t-DCF Cost Parameters
# =============================
Pspoof = 0.05
cost_model = {
    'Pspoof': Pspoof,
    'Ptar': (1 - Pspoof) * 0.99,
    'Pnon': (1 - Pspoof) * 0.01,
    'Cmiss_asv': 1,
    'Cfa_asv': 10,
    'Cmiss_cm': 1,
    'Cfa_cm': 10,
}

# =============================
# Load ASV scores (3-column format)
# =============================
asv_data = np.genfromtxt(asv_score_file, dtype=str)
asv_keys = asv_data[:, 1]                              # 'target', 'nontarget', 'spoof'
asv_scores = asv_data[:, 2].astype(np.float32)

# =============================
# Load CM scores (3-column format)
# =============================
cm_data = np.genfromtxt(cm_score_file, dtype=str)
cm_keys = cm_data[:, 1]                                # 'bonafide', 'spoof'
cm_scores = cm_data[:, 2].astype(np.float32)

# =============================
# Split scores into groups
# =============================
tar_asv   = asv_scores[asv_keys == 'target']
non_asv   = asv_scores[asv_keys == 'nontarget']
spoof_asv = asv_scores[asv_keys == 'spoof']

bona_cm   = cm_scores[cm_keys == 'bonafide']
spoof_cm  = cm_scores[cm_keys == 'spoof']

# =============================
# Compute EERs + Thresholds
# =============================
eer_asv, asv_threshold = em.compute_eer(tar_asv, non_asv)
eer_cm = em.compute_eer(bona_cm, spoof_cm)[0]

Pfa_asv, Pmiss_asv, Pmiss_spoof_asv = em.obtain_asv_error_rates(
    tar_asv, non_asv, spoof_asv, asv_threshold
)

# =============================
# Compute t-DCF
# =============================
tDCF_curve, CM_thresholds = em.compute_tDCF(
    bona_cm, spoof_cm,
    Pfa_asv, Pmiss_asv, Pmiss_spoof_asv,
    cost_model, True
)

min_tDCF_index = np.argmin(tDCF_curve)
min_tDCF = tDCF_curve[min_tDCF_index]

# =============================
# Print Results
# =============================
print('ASV SYSTEM')
print('   EER            = {:8.5f} %'.format(eer_asv * 100))
print('   Pfa            = {:8.5f} %'.format(Pfa_asv * 100))
print('   Pmiss          = {:8.5f} %'.format(Pmiss_asv * 100))
print('   1-Pmiss,spoof  = {:8.5f} %'.format((1 - Pmiss_spoof_asv) * 100))

print('\nCM SYSTEM')
print('   EER            = {:8.5f} %'.format(eer_cm * 100))

print('\nTANDEM')
print('   min-tDCF       = {:8.5f}'.format(min_tDCF))

# =============================
# Plots
# =============================
plt.figure(figsize=(12, 5))
plt.subplot(121)
plt.hist(tar_asv, bins=50, density=True, histtype='step', label='Target')
plt.hist(non_asv, bins=50, density=True, histtype='step', label='Nontarget')
plt.hist(spoof_asv, bins=50, density=True, histtype='step', label='Spoof')
plt.axvline(asv_threshold, color='black', linestyle='dotted', label='EER threshold')
plt.title("ASV Score Histogram")
plt.xlabel("ASV Score")
plt.ylabel("Density")
plt.legend()

plt.subplot(122)
plt.hist(bona_cm, bins=50, density=True, histtype='step', label='Bona fide')
plt.hist(spoof_cm, bins=50, density=True, histtype='step', label='Spoof')
plt.title("CM Score Histogram")
plt.xlabel("CM Score")
plt.ylabel("Density")
plt.legend()

plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 4))
plt.plot(CM_thresholds, tDCF_curve, label='t-DCF')
plt.plot(CM_thresholds[min_tDCF_index], min_tDCF, 'ro', label=f'min t-DCF = {min_tDCF:.5f}')
plt.axhline(1.0, linestyle='--', color='gray', label='t-DCF = 1 (bad CM)')
plt.title('Normalized t-DCF Curve')
plt.xlabel('CM threshold index')
plt.ylabel('Normalized t-DCF')
plt.ylim([0, 1.5])
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""# **Calculating EER and t-DCF for AST Evaluation set**"""

import numpy as np
import matplotlib.pyplot as plt
import eval_metrics as em

# =============================
# Set CM & ASV score file paths
# =============================
cm_score_file = '/content/cm_scores_eval_ast (2).txt'  # 3-column: <utt> <label> <score>
asv_score_file = '/content/ASVspoof2019.LA.asv.eval.gi.trl.scores.txt'  # 3-column official ASV file


# =============================
# t-DCF Cost Parameters
# =============================
Pspoof = 0.05
cost_model = {
    'Pspoof': Pspoof,
    'Ptar': (1 - Pspoof) * 0.99,
    'Pnon': (1 - Pspoof) * 0.01,
    'Cmiss_asv': 1,
    'Cfa_asv': 10,
    'Cmiss_cm': 1,
    'Cfa_cm': 10,
}

# =============================
# Load ASV scores (3-column format)
# =============================
asv_data = np.genfromtxt(asv_score_file, dtype=str)
asv_sources = asv_data[:, 0]
asv_keys = asv_data[:, 1]
asv_scores = asv_data[:, 2].astype(np.float32)

# =============================
# Load CM scores (3-column format)
# =============================
cm_data = np.genfromtxt(cm_score_file, dtype=str)
cm_utt_id = cm_data[:, 0]
cm_keys = cm_data[:, 1]                # 'spoof' or 'bonafide'
cm_scores = cm_data[:, 2].astype(np.float32)

# =============================
# Split scores into groups
# =============================
tar_asv   = asv_scores[asv_keys == 'target']
non_asv   = asv_scores[asv_keys == 'nontarget']
spoof_asv = asv_scores[asv_keys == 'spoof']

bona_cm   = cm_scores[cm_keys == 'bonafide']
spoof_cm  = cm_scores[cm_keys == 'spoof']

# =============================
# Compute EERs + Thresholds
# =============================
eer_asv, asv_threshold = em.compute_eer(tar_asv, non_asv)
eer_cm = em.compute_eer(bona_cm, spoof_cm)[0]

Pfa_asv, Pmiss_asv, Pmiss_spoof_asv = em.obtain_asv_error_rates(
    tar_asv, non_asv, spoof_asv, asv_threshold
)

# =============================
# Compute t-DCF
# =============================
tDCF_curve, CM_thresholds = em.compute_tDCF(
    bona_cm, spoof_cm,
    Pfa_asv, Pmiss_asv, Pmiss_spoof_asv,
    cost_model, True
)

min_tDCF_index = np.argmin(tDCF_curve)
min_tDCF = tDCF_curve[min_tDCF_index]

# =============================
# Print Results
# =============================
print('ASV SYSTEM')
print('   EER            = {:8.5f} %'.format(eer_asv * 100))
print('   Pfa            = {:8.5f} %'.format(Pfa_asv * 100))
print('   Pmiss          = {:8.5f} %'.format(Pmiss_asv * 100))
print('   1-Pmiss,spoof  = {:8.5f} %'.format((1 - Pmiss_spoof_asv) * 100))

print('\nCM SYSTEM')
print('   EER            = {:8.5f} %'.format(eer_cm * 100))

print('\nTANDEM')
print('   min-tDCF       = {:8.5f}'.format(min_tDCF))

# =============================
# Plots
# =============================
plt.figure(figsize=(12, 5))
plt.subplot(121)
plt.hist(tar_asv, bins=50, density=True, histtype='step', label='Target')
plt.hist(non_asv, bins=50, density=True, histtype='step', label='Nontarget')
plt.hist(spoof_asv, bins=50, density=True, histtype='step', label='Spoof')
plt.axvline(asv_threshold, color='black', linestyle='dotted', label='EER threshold')
plt.title("ASV Score Histogram")
plt.xlabel("ASV Score")
plt.ylabel("Density")
plt.legend()

plt.subplot(122)
plt.hist(bona_cm, bins=50, density=True, histtype='step', label='Bona fide')
plt.hist(spoof_cm, bins=50, density=True, histtype='step', label='Spoof')
plt.title("CM Score Histogram")
plt.xlabel("CM Score")
plt.ylabel("Density")
plt.legend()

plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 4))
plt.plot(CM_thresholds, tDCF_curve, label='t-DCF')
plt.plot(CM_thresholds[min_tDCF_index], min_tDCF, 'ro', label=f'min t-DCF = {min_tDCF:.5f}')
plt.axhline(1.0, linestyle='--', color='gray', label='t-DCF = 1 (bad CM)')
plt.title('Normalized t-DCF Curve')
plt.xlabel('CM threshold index')
plt.ylabel('Normalized t-DCF')
plt.ylim([0, 1.5])
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""# **Calculating EER and t-DCF for ConvNext-tiny Development set**"""

import numpy as np
import matplotlib.pyplot as plt
import eval_metrics as em

# =============================
# Set CM & ASV score file paths (DEV)
# =============================
cm_score_file = '/content/cm_scores_dev_convnext (3).txt'      # CM scores: 3-column <utt> <label> <score>
asv_score_file = '/content/asv_dev.txt'               # ASV scores: 3-column <utt> <label> <score>

# =============================
# t-DCF Cost Parameters
# =============================
Pspoof = 0.05
cost_model = {
    'Pspoof': Pspoof,
    'Ptar': (1 - Pspoof) * 0.99,
    'Pnon': (1 - Pspoof) * 0.01,
    'Cmiss_asv': 1,
    'Cfa_asv': 10,
    'Cmiss_cm': 1,
    'Cfa_cm': 10,
}

# =============================
# Load ASV scores (3-column format)
# =============================
asv_data = np.genfromtxt(asv_score_file, dtype=str)
asv_keys = asv_data[:, 1]                              # 'target', 'nontarget', 'spoof'
asv_scores = asv_data[:, 2].astype(np.float32)

# =============================
# Load CM scores (3-column format)
# =============================
cm_data = np.genfromtxt(cm_score_file, dtype=str)
cm_keys = cm_data[:, 1]                                # 'bonafide', 'spoof'
cm_scores = cm_data[:, 2].astype(np.float32)

# =============================
# Split scores into groups
# =============================
tar_asv   = asv_scores[asv_keys == 'target']
non_asv   = asv_scores[asv_keys == 'nontarget']
spoof_asv = asv_scores[asv_keys == 'spoof']

bona_cm   = cm_scores[cm_keys == 'bonafide']
spoof_cm  = cm_scores[cm_keys == 'spoof']

# =============================
# Compute EERs + Thresholds
# =============================
eer_asv, asv_threshold = em.compute_eer(tar_asv, non_asv)
eer_cm = em.compute_eer(bona_cm, spoof_cm)[0]

Pfa_asv, Pmiss_asv, Pmiss_spoof_asv = em.obtain_asv_error_rates(
    tar_asv, non_asv, spoof_asv, asv_threshold
)

# =============================
# Compute t-DCF
# =============================
tDCF_curve, CM_thresholds = em.compute_tDCF(
    bona_cm, spoof_cm,
    Pfa_asv, Pmiss_asv, Pmiss_spoof_asv,
    cost_model, True
)

min_tDCF_index = np.argmin(tDCF_curve)
min_tDCF = tDCF_curve[min_tDCF_index]

# =============================
# Print Results
# =============================
print('ASV SYSTEM')
print('   EER            = {:8.5f} %'.format(eer_asv * 100))
print('   Pfa            = {:8.5f} %'.format(Pfa_asv * 100))
print('   Pmiss          = {:8.5f} %'.format(Pmiss_asv * 100))
print('   1-Pmiss,spoof  = {:8.5f} %'.format((1 - Pmiss_spoof_asv) * 100))

print('\nCM SYSTEM')
print('   EER            = {:8.5f} %'.format(eer_cm * 100))

print('\nTANDEM')
print('   min-tDCF       = {:8.5f}'.format(min_tDCF))

# =============================
# Plots
# =============================
plt.figure(figsize=(12, 5))
plt.subplot(121)
plt.hist(tar_asv, bins=50, density=True, histtype='step', label='Target')
plt.hist(non_asv, bins=50, density=True, histtype='step', label='Nontarget')
plt.hist(spoof_asv, bins=50, density=True, histtype='step', label='Spoof')
plt.axvline(asv_threshold, color='black', linestyle='dotted', label='EER threshold')
plt.title("ASV Score Histogram")
plt.xlabel("ASV Score")
plt.ylabel("Density")
plt.legend()

plt.subplot(122)
plt.hist(bona_cm, bins=50, density=True, histtype='step', label='Bona fide')
plt.hist(spoof_cm, bins=50, density=True, histtype='step', label='Spoof')
plt.title("CM Score Histogram")
plt.xlabel("CM Score")
plt.ylabel("Density")
plt.legend()

plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 4))
plt.plot(CM_thresholds, tDCF_curve, label='t-DCF')
plt.plot(CM_thresholds[min_tDCF_index], min_tDCF, 'ro', label=f'min t-DCF = {min_tDCF:.5f}')
plt.axhline(1.0, linestyle='--', color='gray', label='t-DCF = 1 (bad CM)')
plt.title('Normalized t-DCF Curve')
plt.xlabel('CM threshold index')
plt.ylabel('Normalized t-DCF')
plt.ylim([0, 1.5])
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""# **Calculating EER and t-DCF for ConvNext-tiny Evaluation set**"""

import numpy as np
import matplotlib.pyplot as plt
import eval_metrics as em

# =============================
# Set CM & ASV score file paths
# =============================
cm_score_file = '/content/cm_scores_eval_convnext (1).txt'  # 3-column: <utt> <label> <score>
asv_score_file = '/content/ASVspoof2019.LA.asv.eval.gi.trl.scores.txt'  # 3-column official ASV file


# =============================
# t-DCF Cost Parameters
# =============================
Pspoof = 0.05
cost_model = {
    'Pspoof': Pspoof,
    'Ptar': (1 - Pspoof) * 0.99,
    'Pnon': (1 - Pspoof) * 0.01,
    'Cmiss_asv': 1,
    'Cfa_asv': 10,
    'Cmiss_cm': 1,
    'Cfa_cm': 10,
}

# =============================
# Load ASV scores (3-column format)
# =============================
asv_data = np.genfromtxt(asv_score_file, dtype=str)
asv_sources = asv_data[:, 0]
asv_keys = asv_data[:, 1]
asv_scores = asv_data[:, 2].astype(np.float32)

# =============================
# Load CM scores (3-column format)
# =============================
cm_data = np.genfromtxt(cm_score_file, dtype=str)
cm_utt_id = cm_data[:, 0]
cm_keys = cm_data[:, 1]                # 'spoof' or 'bonafide'
cm_scores = cm_data[:, 2].astype(np.float32)

# =============================
# Split scores into groups
# =============================
tar_asv   = asv_scores[asv_keys == 'target']
non_asv   = asv_scores[asv_keys == 'nontarget']
spoof_asv = asv_scores[asv_keys == 'spoof']

bona_cm   = cm_scores[cm_keys == 'bonafide']
spoof_cm  = cm_scores[cm_keys == 'spoof']

# =============================
# Compute EERs + Thresholds
# =============================
eer_asv, asv_threshold = em.compute_eer(tar_asv, non_asv)
eer_cm = em.compute_eer(bona_cm, spoof_cm)[0]

Pfa_asv, Pmiss_asv, Pmiss_spoof_asv = em.obtain_asv_error_rates(
    tar_asv, non_asv, spoof_asv, asv_threshold
)

# =============================
# Compute t-DCF
# =============================
tDCF_curve, CM_thresholds = em.compute_tDCF(
    bona_cm, spoof_cm,
    Pfa_asv, Pmiss_asv, Pmiss_spoof_asv,
    cost_model, True
)

min_tDCF_index = np.argmin(tDCF_curve)
min_tDCF = tDCF_curve[min_tDCF_index]

# =============================
# Print Results
# =============================
print('ASV SYSTEM')
print('   EER            = {:8.5f} %'.format(eer_asv * 100))
print('   Pfa            = {:8.5f} %'.format(Pfa_asv * 100))
print('   Pmiss          = {:8.5f} %'.format(Pmiss_asv * 100))
print('   1-Pmiss,spoof  = {:8.5f} %'.format((1 - Pmiss_spoof_asv) * 100))

print('\nCM SYSTEM')
print('   EER            = {:8.5f} %'.format(eer_cm * 100))

print('\nTANDEM')
print('   min-tDCF       = {:8.5f}'.format(min_tDCF))

# =============================
# Plots
# =============================
plt.figure(figsize=(12, 5))
plt.subplot(121)
plt.hist(tar_asv, bins=50, density=True, histtype='step', label='Target')
plt.hist(non_asv, bins=50, density=True, histtype='step', label='Nontarget')
plt.hist(spoof_asv, bins=50, density=True, histtype='step', label='Spoof')
plt.axvline(asv_threshold, color='black', linestyle='dotted', label='EER threshold')
plt.title("ASV Score Histogram")
plt.xlabel("ASV Score")
plt.ylabel("Density")
plt.legend()

plt.subplot(122)
plt.hist(bona_cm, bins=50, density=True, histtype='step', label='Bona fide')
plt.hist(spoof_cm, bins=50, density=True, histtype='step', label='Spoof')
plt.title("CM Score Histogram")
plt.xlabel("CM Score")
plt.ylabel("Density")
plt.legend()

plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 4))
plt.plot(CM_thresholds, tDCF_curve, label='t-DCF')
plt.plot(CM_thresholds[min_tDCF_index], min_tDCF, 'ro', label=f'min t-DCF = {min_tDCF:.5f}')
plt.axhline(1.0, linestyle='--', color='gray', label='t-DCF = 1 (bad CM)')
plt.title('Normalized t-DCF Curve')
plt.xlabel('CM threshold index')
plt.ylabel('Normalized t-DCF')
plt.ylim([0, 1.5])
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()