{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63a7690-0e81-47d2-9538-7de83c3df7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 4090 D\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "from transformers import ASTFeatureExtractor, ASTForAudioClassification\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870067bb-713e-4e6b-9001-26f34935124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {\"bonafide\": 1, \"spoof\": 0}\n",
    "sampling_rate = 16000\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(\"/root/autodl-fs/ast-extractor\")\n",
    "\n",
    "class ASTDataset(Dataset):\n",
    "    def __init__(self, csv_path, audio_dir, nrows=None):\n",
    "        self.df = pd.read_csv(csv_path, sep=\" \", header=None, nrows=nrows)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.labels = [LABEL_MAP[label] for label in self.df[4]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        audio_filename = row[1] + \".flac\"\n",
    "        label = LABEL_MAP[row[4]]\n",
    "        flac_path = os.path.join(self.audio_dir, audio_filename)\n",
    "        waveform, sr = torchaudio.load(flac_path)\n",
    "        waveform = waveform.mean(dim=0)\n",
    "        if sr != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(sr, 16000)\n",
    "            waveform = resampler(waveform)\n",
    "        return {\"waveform\": waveform.numpy(), \"labels\": torch.tensor(label),  \"utt_id\": row[1]}\n",
    "\n",
    "def ast_collate_fn(batch):\n",
    "    waveforms = [item[\"waveform\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    utt_ids = [x[\"utt_id\"] for x in batch]\n",
    "    inputs = feature_extractor(waveforms, sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
    "    return {\"input_values\": inputs[\"input_values\"], \"labels\": torch.stack(labels),  \"utt_id\": utt_ids}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6db436-5795-41ce-b3ec-225d9386f450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 128])\n",
      "tensor([1, 0, 0, 0, 1, 0, 1, 1])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "eval_dataset = ASTDataset(\n",
    "    csv_path=\"/root/autodl-fs/ASVspoof2019.LA.cm.eval.trl.txt\",\n",
    "    audio_dir=\"/root/autodl-tmp/ASVspoof2019_LA_eval/flac\",\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8, shuffle=True, collate_fn=ast_collate_fn)\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    print(batch[\"input_values\"].shape)  \n",
    "    print(batch[\"labels\"])\n",
    "    break\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2873ef-48dd-4f5c-851d-041cb96d223c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at /root/autodl-fs/ast-audioset and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at /root/autodl-fs/ast-audioset and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at /root/autodl-fs/ast-audioset and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at /root/autodl-fs/ast-audioset and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at /root/autodl-fs/ast-audioset and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at /root/autodl-fs/ast-audioset and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/root/autodl-fs/ast-audioset\"\n",
    "model_lp_wl = ASTForAudioClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,                      \n",
    "    ignore_mismatched_sizes=True    \n",
    ").to(device)  \n",
    "model_pt_wl = ASTForAudioClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,                     \n",
    "    ignore_mismatched_sizes=True      \n",
    ").to(device)\n",
    "model_ft_wl = ASTForAudioClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,                      \n",
    "    ignore_mismatched_sizes=True      \n",
    ").to(device)\n",
    "model_lp_ws = ASTForAudioClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,                      \n",
    "    ignore_mismatched_sizes=True      \n",
    ").to(device)\n",
    "model_pt_ws = ASTForAudioClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,                      \n",
    "    ignore_mismatched_sizes=True      \n",
    ").to(device)\n",
    "model_ft_ws = ASTForAudioClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,                      \n",
    "    ignore_mismatched_sizes=True      \n",
    ").to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0958a44d-797e-4273-afce-609a6e8c5c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_lp_wl = \"/root/autodl-tmp/ast_wl_lp/epoch_20.pt\"\n",
    "model_lp_wl.load_state_dict(torch.load(model_path_lp_wl))\n",
    "model_path_pt_wl = \"/root/autodl-tmp/ast_wl_partial/epoch_10.pt\"\n",
    "model_pt_wl.load_state_dict(torch.load(model_path_pt_wl))\n",
    "model_path_ft_wl = \"/root/autodl-tmp/ast_wl_fully/epoch_7.pt\"\n",
    "model_ft_wl.load_state_dict(torch.load(model_path_ft_wl))\n",
    "model_path_lp_ws = \"/root/autodl-tmp/ast_ws_lp/epoch_20.pt\"\n",
    "\n",
    "model_lp_ws.load_state_dict(torch.load(model_path_lp_ws))\n",
    "model_path_pt_ws = \"/root/autodl-tmp/ast_ws_partial/epoch_15.pt\"\n",
    "model_pt_ws.load_state_dict(torch.load(model_path_pt_ws))\n",
    "model_path_ft_ws = \"/root/autodl-tmp/ast_ws_fully/epoch_5.pt\"\n",
    "model_ft_ws.load_state_dict(torch.load(model_path_ft_ws))\n",
    "\n",
    "# Put all models in evaluation mode\n",
    "model_lp_wl.eval()\n",
    "model_pt_wl.eval()\n",
    "model_ft_wl.eval()\n",
    "model_lp_ws.eval()\n",
    "model_pt_ws.eval()\n",
    "model_ft_ws.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04816a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "def write_model_predictions(model, dataloader, device, output_csv_path):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    all_results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"Evaluating {output_csv_path}\"):\n",
    "            x = batch[\"input_values\"].to(device)\n",
    "            y = batch[\"labels\"].to(device)\n",
    "            utt_ids = batch[\"utt_id\"]  \n",
    "\n",
    "            output = model(x)           \n",
    "            logits = output.logits      \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            for uid, label, pred, logit in zip(\n",
    "                utt_ids, y.cpu().numpy(), preds.cpu().numpy(), logits.cpu().numpy()\n",
    "            ):\n",
    "                all_results.append({\n",
    "                    'id': uid,\n",
    "                    'label': int(label),\n",
    "                    'prediction': int(pred),\n",
    "                    'logit_bonafide': float(logit[0]),\n",
    "                    'logit_spoof': float(logit[1])\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"✅ Saved predictions to {output_csv_path}\")\n",
    "\n",
    "\n",
    "# === Example usage for multiple models ===\n",
    "model_list = {\n",
    "    'model_lp_wl':model_lp_wl,\n",
    "    'model_pt_wl':model_pt_wl,\n",
    "    'model_ft_wl':model_ft_wl,\n",
    "    'model_lp_ws':model_lp_ws,\n",
    "    'model_pt_ws':model_pt_ws,\n",
    "    'model_ft_ws':model_ft_ws\n",
    "}\n",
    "\n",
    "for model_name, model in model_list.items():\n",
    "    csv_name = f\"ast_{model_name}_eval_predictions.csv\"\n",
    "    write_model_predictions(model, eval_dataloader, device, csv_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f67092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import det_curve\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/root/autodl-tmp/convnext_result/conv_model_conv_dev_predictions.csv\")\n",
    "\n",
    "y_true = df[\"label\"].values  # 1 = bonafide, 0 = spoof\n",
    "logits = df[[\"logit_spoof\", \"logit_bonafide\"]].values.astype(np.float32)\n",
    "y_pred = df[\"prediction\"].values\n",
    "accuracy = (y_true == y_pred).mean()\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.6f}\")\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "probs = softmax(logits)\n",
    "y_score = probs[:, 1]  \n",
    "\n",
    "\n",
    "fpr, fnr, thresholds = det_curve(y_true, y_score, pos_label=1)\n",
    "\n",
    "eer_idx = np.nanargmin(np.abs(fpr - fnr))\n",
    "eer = (fpr[eer_idx] + fnr[eer_idx]) / 2\n",
    "eer_threshold = thresholds[eer_idx]\n",
    "\n",
    "\n",
    "cost_model = {\n",
    "    'Ptar': 0.99 * (1 - 0.05),\n",
    "    'Pnon': 0.01 * (1 - 0.05),\n",
    "    'Pspoof': 0.05,\n",
    "    'Cmiss_asv': 1,\n",
    "    'Cfa_asv': 10,\n",
    "    'Cmiss_cm': 1,\n",
    "    'Cfa_cm': 10\n",
    "}\n",
    "\n",
    "# Fixed ASV errors (standard practice for CM-only t-DCF)\n",
    "Pmiss_asv = 0.01\n",
    "Pfa_asv = 0.01\n",
    "Pmiss_spoof_asv = 0.05\n",
    "\n",
    "# t-DCF at EER threshold\n",
    "def compute_tdcf_at_threshold(y_true, y_score, threshold, cost_model):\n",
    "    y_pred = (y_score >= threshold).astype(int)\n",
    "    miss = np.sum((y_pred == 0) & (y_true == 1)) / np.sum(y_true == 1)\n",
    "    fa = np.sum((y_pred == 1) & (y_true == 0)) / np.sum(y_true == 0)\n",
    "    C1 = cost_model['Ptar'] * (cost_model['Cmiss_cm'] - cost_model['Cmiss_asv'] * Pmiss_asv) - \\\n",
    "         cost_model['Pnon'] * cost_model['Cfa_asv'] * Pfa_asv\n",
    "    C2 = cost_model['Cfa_cm'] * cost_model['Pspoof'] * (1 - Pmiss_spoof_asv)\n",
    "    tdcf = C1 * miss + C2 * fa\n",
    "    return tdcf / min(C1, C2)\n",
    "\n",
    "tdcf_at_eer = compute_tdcf_at_threshold(y_true, y_score, eer_threshold, cost_model)\n",
    "\n",
    "# min-tDCF over all thresholds\n",
    "def compute_min_tdcf(fpr, fnr, cost_model):\n",
    "    C1 = cost_model['Ptar'] * (cost_model['Cmiss_cm'] - cost_model['Cmiss_asv'] * Pmiss_asv) - \\\n",
    "         cost_model['Pnon'] * cost_model['Cfa_asv'] * Pfa_asv\n",
    "    C2 = cost_model['Cfa_cm'] * cost_model['Pspoof'] * (1 - Pmiss_spoof_asv)\n",
    "    tdcf_curve = C1 * fnr + C2 * fpr\n",
    "    return np.min(tdcf_curve / min(C1, C2))\n",
    "\n",
    "min_tdcf = compute_min_tdcf(fpr, fnr, cost_model)\n",
    "\n",
    "print(\"\\n CM-only Evaluation\")\n",
    "print(f\"EER          : {eer:.6f}\")\n",
    "print(f\"t-DCF@EER    : {tdcf_at_eer:.6f}\")\n",
    "print(f\"min-tDCF     : {min_tdcf:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
